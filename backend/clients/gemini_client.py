"""
Gemini LLM ν΄λΌμ΄μ–ΈνΈ

π΅ A6000 λ§μ΄κ·Έλ μ΄μ… λ€μƒ
- ν„μ¬: Google Gemini API (λ¬΄λ£ ν‹°μ–΄)
- ν–¥ν›„: A6000 μ„λ²„μ LLaMA/Gemma (λ¬΄λ£, λ΅μ»¬)
"""

import os
import json
from typing import Optional
import google.generativeai as genai
from clients.base import LLMClient


class GeminiClient(LLMClient):
    """
    Google Gemini API ν΄λΌμ΄μ–ΈνΈ (κΌ¬λ¦¬μ§λ¬Έ μƒμ„±μ©)

    β οΈ A6000 μ„λ²„λ΅ λ§μ΄κ·Έλ μ΄μ… μ‹ LLaMAA6000Clientλ΅ κµμ²΄

    ν™κ²½ λ³€μ:
        GEMINI_API_KEY: Google Gemini API ν‚¤
        GEMINI_MODEL: λ¨λΈ μ΄λ¦„ (κΈ°λ³Έ: gemini-2.0-flash-exp)
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model_name: Optional[str] = None
    ):
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY is required")

        genai.configure(api_key=self.api_key)

        self.model_name = model_name or os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp")
        self.model = genai.GenerativeModel(self.model_name)

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 200,
        temperature: float = 0.7
    ) -> str:
        """
        ν”„λ΅¬ν”„νΈλ¥Ό λ°›μ•„ ν…μ¤νΈ μƒμ„±

        Args:
            prompt: μƒμ„± ν”„λ΅¬ν”„νΈ
            max_tokens: μµλ€ ν† ν° μ
            temperature: μƒμ„± μ¨λ„

        Returns:
            μƒμ„±λ ν…μ¤νΈ
        """
        generation_config = genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature
        )

        response = self.model.generate_content(
            prompt,
            generation_config=generation_config
        )

        return response.text.strip()

    def build_followup_prompt(
        self,
        portfolio_text: str,
        current_question: str,
        user_answer: str,
        question_type: str = "technical"
    ) -> str:
        """
        κΌ¬λ¦¬μ§λ¬Έ μƒμ„±μ„ μ„ν• ν”„λ΅¬ν”„νΈ κµ¬μ„±
        
        Args:
            portfolio_text: ν¬νΈν΄λ¦¬μ¤ νμ‹± ν…μ¤νΈ
            current_question: ν„μ¬ μ§λ¬Έ
            user_answer: μ‚¬μ©μ λ‹µλ³€ (STT κ²°κ³Ό)
            question_type: μ§λ¬Έ μ ν• (technical, behavioral, project)
            
        Returns:
            κΌ¬λ¦¬μ§λ¬Έ μƒμ„± ν”„λ΅¬ν”„νΈ
        """
        prompt = f"""λ‹Ήμ‹ μ€ κ²½ν— λ§μ€ κ°λ°μ λ©΄μ ‘κ΄€μ…λ‹λ‹¤.
μ§€μ›μμ ν¬νΈν΄λ¦¬μ¤μ™€ λ‹µλ³€μ„ κΈ°λ°μΌλ΅ κΉμ΄ μλ” κΌ¬λ¦¬μ§λ¬Έ 1κ°λ¥Ό μƒμ„±ν•΄μ£Όμ„Έμ”.

# ν¬νΈν΄λ¦¬μ¤ μ •λ³΄
{portfolio_text[:1000]}  # λ„λ¬΄ κΈΈλ©΄ μλΌλƒ„

# ν„μ¬ μ§λ¬Έ
{current_question}

# μ§€μ›μ λ‹µλ³€
{user_answer}

# μ”κµ¬μ‚¬ν•­
1. μ§€μ›μμ λ‹µλ³€μ—μ„ **κµ¬μ²΄μ μΈ κ²½ν—**μ„ λ” νκ³ λ“¤μ–΄μ•Ό ν•©λ‹λ‹¤
2. ν¬νΈν΄λ¦¬μ¤μ— μ–ΈκΈ‰λ κΈ°μ  μ¤νƒκ³Ό μ—°κ΄€μ§€μ–΄ μ§λ¬Έν•μ„Έμ”
3. λ‹¨λ‹µν•μ΄ μ•„λ‹, κ²½ν—κ³Ό μƒκ°μ„ μ΄λμ–΄λ‚Ό μ μλ” μ§λ¬Έμ΄μ–΄μ•Ό ν•©λ‹λ‹¤
4. μ§λ¬Έμ€ **1λ¬Έμ¥**μΌλ΅ κ°„κ²°ν•κ² μ‘μ„±ν•μ„Έμ”
5. μ΅΄λ“λ§μ„ μ‚¬μ©ν•μ„Έμ” (μ: "~ν•μ…¨λ‚μ”?", "~ν•μ‹  κ²½ν—μ΄ μλ‚μ”?")

# μ¶λ ¥ ν•μ‹
κΌ¬λ¦¬μ§λ¬Έλ§ μ¶λ ¥ν•μ„Έμ”. μ¶”κ°€ μ„¤λ…μ΄λ‚ λ§ν¬λ‹¤μ΄ μ—†μ΄ μ§λ¬Έ 1κ°λ§ μ‘μ„±ν•μ„Έμ”.

μμ‹:
- "λ°©κΈ λ§μ”€ν•μ‹  λ§μ΄ν¬λ΅μ„λΉ„μ¤ μ•„ν‚¤ν…μ²μ—μ„ μ„λΉ„μ¤ κ°„ ν†µμ‹ μ€ μ–΄λ–»κ² κµ¬ν„ν•μ…¨λ‚μ”?"
- "ν¬νΈν΄λ¦¬μ¤μ— μλ” Dockerλ¥Ό μ‹¤μ  ν”„λ΅μ νΈμ—μ„ μ–΄λ–»κ² ν™μ©ν•μ…¨λ‚μ”?"
"""
        return prompt

    async def generate_initial_questions(
        self,
        portfolio_text: str,
        job_posting_text: str
    ) -> str:
        """
        μ΄κΈ° λ©΄μ ‘ μ§λ¬Έ 3κ°λ¥Ό μƒμ„±ν•©λ‹λ‹¤. (μ•½μ , ν¬νΈν΄λ¦¬μ¤ κ²€μ¦, μ§λ¬΄ μ—­λ‰)

        Args:
            portfolio_text: ν¬νΈν΄λ¦¬μ¤ λ‚΄μ©
            job_posting_text: μ§λ¬΄ κ³µκ³  λ‚΄μ©

        Returns:
            JSON λ¬Έμμ—΄ (μ§λ¬Έ λ¦¬μ¤νΈ)
        """
        prompt = f"""
λ‹Ήμ‹ μ€ μ „λ¬Έ κΈ°μ  λ©΄μ ‘κ΄€μ…λ‹λ‹¤.
μ§€μ›μμ ν¬νΈν΄λ¦¬μ¤μ™€ μ±„μ© κ³µκ³ λ¥Ό λ°”νƒ•μΌλ΅ λ©΄μ ‘ μ§λ¬Έ 3κ°λ¥Ό μƒμ„±ν•΄μ£Όμ„Έμ”.

# μ±„μ© κ³µκ³ 
{job_posting_text}

# μ§€μ›μ ν¬νΈν΄λ¦¬μ¤
{portfolio_text}

# μ§λ¬Έ μƒμ„± κ·μΉ™ (λ°λ“μ‹ μ•„λ 3κ°€μ§€ μ ν•μΌλ΅ ν•λ‚μ”© μƒμ„±)

1. μ•½μ  μ§λ¬Έ (Weakness)
   - μ •μ: μ§λ¬΄(μ±„μ© κ³µκ³ )μ—μ„λ” μ¤‘μ”ν•κ² μ”κµ¬ν•μ§€λ§, ν¬νΈν΄λ¦¬μ¤μ—λ” λ“λ¬λ‚μ§€ μ•κ±°λ‚ λ¶€μ΅±ν•΄ λ³΄μ΄λ” μ—­λ‰μ— λ€ν• μ§λ¬Έμ…λ‹λ‹¤.
   - κΈ°μ¤€: λ°λ“μ‹ 'μ§λ¬΄ κ³µκ³ 'λ¥Ό κΈ°μ¤€μΌλ΅ νλ‹¨ν•μ„Έμ”.

2. ν¬νΈν΄λ¦¬μ¤ κ²€μ¦ μ§λ¬Έ (Portfolio Verification)
   - μ •μ: ν¬νΈν΄λ¦¬μ¤μ— κΈ°μ¬λ ν”„λ΅μ νΈ κ²½ν—, μ„±κ³Ό, κΈ°μ  μ‚¬μ©μ— λ€ν• μ‚¬μ‹¤ μ—¬λ¶€μ™€ κΉμ΄λ¥Ό κ²€μ¦ν•λ” μ§λ¬Έμ…λ‹λ‹¤.
   - λ‚΄μ©: ν¬νΈν΄λ¦¬μ¤μ κµ¬μ²΄μ μΈ λ‚΄μ©μ„ μ–ΈκΈ‰ν•λ©° μ§λ¬Έν•μ„Έμ”.

3. μ§λ¬΄ κ΄€λ ¨ μ§λ¬Έ (Job Competency)
   - μ •μ: μ—…λ΅λ“ν• μ§λ¬΄ κ³µκ³ μ—μ„ μ”κµ¬ν•λ” ν•µμ‹¬ μ—­λ‰ μ „λ°μ— λ€ν• μ§λ¬Έμ…λ‹λ‹¤.
   - λ‚΄μ©: ν•΄λ‹Ή μ§λ¬΄λ¥Ό μν–‰ν•κΈ° μ„ν•΄ ν•„μμ μΈ μ§€μ‹μ΄λ‚ λ¬Έμ  ν•΄κ²° λ¥λ ¥μ„ λ¬»μµλ‹λ‹¤.

# μ¶λ ¥ ν•μ‹ (λ°λ“μ‹ JSON λ°°μ—΄λ΅λ§ μ‘λ‹µ, λ§ν¬λ‹¤μ΄ μ—†μ΄)
[
  {{
    "type": "weakness",
    "text": "μ§λ¬Έ λ‚΄μ©"
  }},
  {{
    "type": "portfolio",
    "text": "μ§λ¬Έ λ‚΄μ©"
  }},
  {{
    "type": "job",
    "text": "μ§λ¬Έ λ‚΄μ©"
  }}
]
"""
        return await self.generate(prompt, max_tokens=1000)


class LLaMAA6000Client(LLMClient):
    """
    A6000 μ„λ²„μ LLaMA/Gemma ν΄λΌμ΄μ–ΈνΈ

    β… μµμΆ… λ°°ν¬μ©

    ν™κ²½ λ³€μ:
        A6000_LLM_URL: A6000 LLM μ„λ²„ URL (μ: http://a6000-server:8003)
    """

    def __init__(self, base_url: Optional[str] = None):
        self.base_url = base_url or os.getenv("A6000_LLM_URL")
        if not self.base_url:
            raise ValueError("A6000_LLM_URL is required")

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 200,
        temperature: float = 0.7
    ) -> str:
        """
        A6000 μ„λ²„μ LLMμΌλ΅ ν…μ¤νΈ μƒμ„±

        Args:
            prompt: μƒμ„± ν”„λ΅¬ν”„νΈ
            max_tokens: μµλ€ ν† ν° μ
            temperature: μƒμ„± μ¨λ„

        Returns:
            μƒμ„±λ ν…μ¤νΈ
        """
        import aiohttp

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/generate",
                json={
                    "prompt": prompt,
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
            ) as resp:
                resp.raise_for_status()
                result = await resp.json()
                return result.get("text", "")

    def build_followup_prompt(
        self,
        portfolio_text: str,
        current_question: str,
        user_answer: str,
        question_type: str = "technical"
    ) -> str:
        """GeminiClientμ™€ λ™μΌν• ν”„λ΅¬ν”„νΈ λΉλ”"""
        # GeminiClientμ™€ λ™μΌν• λ΅μ§
        prompt = f"""λ‹Ήμ‹ μ€ κ²½ν— λ§μ€ κ°λ°μ λ©΄μ ‘κ΄€μ…λ‹λ‹¤.
μ§€μ›μμ ν¬νΈν΄λ¦¬μ¤μ™€ λ‹µλ³€μ„ κΈ°λ°μΌλ΅ κΉμ΄ μλ” κΌ¬λ¦¬μ§λ¬Έ 1κ°λ¥Ό μƒμ„±ν•΄μ£Όμ„Έμ”.

# ν¬νΈν΄λ¦¬μ¤ μ •λ³΄
{portfolio_text[:1000]}

# ν„μ¬ μ§λ¬Έ
{current_question}

# μ§€μ›μ λ‹µλ³€
{user_answer}

# μ”κµ¬μ‚¬ν•­
1. μ§€μ›μμ λ‹µλ³€μ—μ„ **κµ¬μ²΄μ μΈ κ²½ν—**μ„ λ” νκ³ λ“¤μ–΄μ•Ό ν•©λ‹λ‹¤
2. ν¬νΈν΄λ¦¬μ¤μ— μ–ΈκΈ‰λ κΈ°μ  μ¤νƒκ³Ό μ—°κ΄€μ§€μ–΄ μ§λ¬Έν•μ„Έμ”
3. λ‹¨λ‹µν•μ΄ μ•„λ‹, κ²½ν—κ³Ό μƒκ°μ„ μ΄λμ–΄λ‚Ό μ μλ” μ§λ¬Έμ΄μ–΄μ•Ό ν•©λ‹λ‹¤
4. μ§λ¬Έμ€ **1λ¬Έμ¥**μΌλ΅ κ°„κ²°ν•κ² μ‘μ„±ν•μ„Έμ”
5. μ΅΄λ“λ§μ„ μ‚¬μ©ν•μ„Έμ”

# μ¶λ ¥
κΌ¬λ¦¬μ§λ¬Έλ§ μ¶λ ¥ν•μ„Έμ”.
"""
        return prompt
