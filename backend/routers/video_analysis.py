"""
Video Analysis Router
ë©´ì ‘ ì˜ìƒ ë¶„ì„ ë° í”¼ë“œë°± ì œê³µ
"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Depends, Form
from sqlalchemy.orm import Session
from pathlib import Path
import soundfile as sf
import os
import json
import shutil
from datetime import datetime
from typing import Optional

from database import get_db
from models import InterviewVideo, InterviewTranscript, NonverbalMetrics, NonverbalTimeline, Feedback, InterviewSession, InterviewQuestion
from pipeline.video_io import extract_frames_opencv, extract_audio_ffmpeg
from pipeline.vision_mediapipe import build_timeline_from_frames, save_timeline
from pipeline.metrics import (
    center_gaze_ratio, smile_ratio, nod_count, emotion_distribution, get_primary_emotion,
    compute_metadata
)
from pipeline.audio_analysis import transcribe_whisper, compute_wpm, compute_filler_count
from pipeline.feedback_generator import generate_feedback_with_gemini, generate_feedback_fallback
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

router = APIRouter()

# ë¹„ë””ì˜¤ ì €ì¥ ë””ë ‰í† ë¦¬
VIDEO_UPLOAD_DIR = Path("uploads/videos")
VIDEO_UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# Gemini API ì‚¬ìš© ì—¬ë¶€ í™•ì¸ (.env ë¡œë“œ í›„)
# GEMINI_API_KEY1, GEMINI_API_KEY2, GEMINI_API_KEY3 ë˜ëŠ” GEMINI_API_KEY ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì‚¬ìš©
# ì‹¤ì œ í”¼ë“œë°± ìƒì„± ì‹œì—ëŠ” generate_feedback_with_gemini()ì—ì„œ í‚¤ 1, 2, 3ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
USE_GEMINI = any(
    os.getenv(f"GEMINI_API_KEY{i}") for i in range(1, 4)
) or bool(os.getenv("GEMINI_API_KEY"))


@router.get("/status")
def video_status():
    """API ìƒíƒœ í™•ì¸"""
    return {
        "gemini_api_enabled": USE_GEMINI,
        "feedback_mode": "AI-powered (Gemini 2.5 Flash Lite)" if USE_GEMINI else "Rule-based",
        "upload_directory": str(VIDEO_UPLOAD_DIR.resolve())
    }


@router.post("/upload")
async def upload_video(
    file: UploadFile = File(...),
    user_id: str = Form(...),
    session_id: str = Form(...),
    question_id: str = Form(...),
    db: Session = Depends(get_db)
):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ ì—…ë¡œë“œ ë° DB ì €ì¥
    
    Args:
        file: ì—…ë¡œë“œí•  ë¹„ë””ì˜¤ íŒŒì¼ (.mp4, .webm, .mov)
        user_id: ì‚¬ìš©ì ID
        session_id: ë©´ì ‘ ì„¸ì…˜ ID
        question_id: ë©´ì ‘ ì§ˆë¬¸ ID
    
    Returns:
        video_id, file_path ë“±
    """
    # íŒŒì¼ í™•ì¥ì ê²€ì¦
    allowed_extensions = {".mp4", ".webm", ".mov", ".avi"}
    file_ext = Path(file.filename).suffix.lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file format: {file_ext}. Allowed: {allowed_extensions}"
        )
    
    try:
        # FK ê²€ì¦: session_idì™€ question_idê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        session = db.query(InterviewSession).filter(InterviewSession.id == session_id).first()
        if not session:
            raise HTTPException(
                status_code=404,
                detail=f"InterviewSession not found: {session_id}. Please create a session first."
            )
        
        question = db.query(InterviewQuestion).filter(InterviewQuestion.id == question_id).first()
        if not question:
            raise HTTPException(
                status_code=404,
                detail=f"InterviewQuestion not found: {question_id}. Please create a question first."
            )
        
        # session_idê°€ í•´ë‹¹ user_idì— ì†í•˜ëŠ”ì§€ í™•ì¸
        if session.user_id != user_id:
            raise HTTPException(
                status_code=403,
                detail=f"Session {session_id} does not belong to user {user_id}"
            )
        
        # ê³ ìœ  íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        unique_filename = f"{user_id}_{session_id}_{timestamp}{file_ext}"
        video_path = VIDEO_UPLOAD_DIR / unique_filename
        
        # íŒŒì¼ ì €ì¥
        with open(video_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # ë¹„ë””ì˜¤ ê¸¸ì´ ì¶”ì¶œ
        import cv2
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
        duration_sec = frame_count / fps if fps > 0 else 0
        cap.release()
        
        # DBì— ì €ì¥
        video_record = InterviewVideo(
            user_id=user_id,
            session_id=session_id,
            question_id=question_id,
            video_url=str(video_path),
            duration_sec=float(duration_sec)
        )
        db.add(video_record)
        db.commit()
        db.refresh(video_record)
        
        return {
            "video_id": video_record.id,
            "filename": unique_filename,
            "file_path": str(video_path),
            "duration_sec": duration_sec,
            "created_at": video_record.created_at
        }
        
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì „ë‹¬
        raise
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒì‹œ ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œ
        if 'video_path' in locals() and video_path.exists():
            video_path.unlink()
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@router.post("/analyze/{video_id}")
def analyze_interview(video_id: str, db: Session = Depends(get_db)):
    """
    ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ ë¶„ì„ ë° AI í”¼ë“œë°± ìƒì„± + DB ì €ì¥
    
    Args:
        video_id: InterviewVideo ID
    
    Environment Variables:
        - GEMINI_API_KEY: Gemini API í‚¤ (ì„¤ì •ì‹œ AI í”¼ë“œë°± ì‚¬ìš©)
    
    Returns:
        - ë¶„ì„ ê²°ê³¼ + DBì— ì €ì¥ëœ ë ˆì½”ë“œ IDs
    """
    # 1. DBì—ì„œ ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ
    video_record = db.query(InterviewVideo).filter(InterviewVideo.id == video_id).first()
    if not video_record:
        raise HTTPException(status_code=404, detail=f"Video not found: {video_id}")
    
    video_path = Path(video_record.video_url)
    if not video_path.exists():
        raise HTTPException(
            status_code=404,
            detail=f"Video file not found: {video_path}"
        )
    
    try:
        # 2. ë¹„ë””ì˜¤ ë¶„í•´
        print(f"ğŸ¬ Processing video: {video_path}")
        
        artifacts_dir = Path("artifacts") / video_id
        frames_dir = artifacts_dir / "frames"
        
        FPS_ANALYZED = 5.0  # Store for metadata
        frames = extract_frames_opencv(
            video_path, fps=FPS_ANALYZED, out_dir=frames_dir
        )

        # 3. Vision timeline ìƒì„±
        print("ğŸ‘ï¸ Analyzing facial features...")
        timeline = build_timeline_from_frames(frames)
        timeline_path = artifacts_dir / "timeline.json"
        save_timeline(timeline, timeline_path)

        # 4. ì˜¤ë””ì˜¤ ë¶„ì„
        print("ğŸ¤ Analyzing audio...")
        wav_path = artifacts_dir / "audio.wav"
        wav = extract_audio_ffmpeg(video_path, wav_path)
        audio, sr = sf.read(str(wav))
        duration_sec = len(audio) / sr
        
        print("ğŸ“ Transcribing speech...")
        WHISPER_MODEL_SIZE = "base"  # Store for metadata
        stt = transcribe_whisper(wav, model_size=WHISPER_MODEL_SIZE)
        text = stt["text"]

        # 5. ë©”íŠ¸ë¦­ ê³„ì‚°
        print("ğŸ“Š Computing metrics...")
        emotion_dist = emotion_distribution(timeline)
        primary_emo = get_primary_emotion(timeline)
        
        # Calculate smile_ratio and capture threshold used
        smile_ratio_val, smile_threshold_used = smile_ratio(timeline, threshold=None)
        
        # Nod pitch threshold
        NOD_PITCH_THRESHOLD = 8.0
        
        metrics = {
            "center_gaze_ratio": center_gaze_ratio(timeline),
            "smile_ratio": smile_ratio_val,
            "nod_count": nod_count(timeline, pitch_thresh_deg=NOD_PITCH_THRESHOLD),
            "emotion_distribution": emotion_dist,
            "primary_emotion": primary_emo,
            "wpm": compute_wpm(text, duration_sec),
            "filler_count": compute_filler_count(text),
        }
        
        # 5.5. ë©”íƒ€ë°ì´í„° ê³„ì‚°
        print("ğŸ“‹ Computing metadata...")
        metadata = compute_metadata(
            timeline=timeline,
            fps_analyzed=FPS_ANALYZED,
            smile_threshold=smile_threshold_used,
            nod_pitch_threshold=NOD_PITCH_THRESHOLD,
            whisper_model_size=WHISPER_MODEL_SIZE,
            duration_sec=duration_sec
        )

        # 6. í”¼ë“œë°± ìƒì„±
        if USE_GEMINI:
            print("ğŸ¤– Generating feedback with Gemini 2.5 Flash Lite...")
            try:
                feedback_list = generate_feedback_with_gemini(metrics, transcript=text)
                feedback_mode = "gemini"
            except Exception as e:
                print(f"âš ï¸ Gemini failed, using fallback: {e}")
                feedback_list = generate_feedback_fallback(metrics)
                feedback_mode = "rule-based"
        else:
            print("ğŸ“ Generating feedback with rule-based system...")
            feedback_list = generate_feedback_fallback(metrics)
            feedback_mode = "rule-based"

        # 7. DBì— ì €ì¥ (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥)
        print("ğŸ’¾ Saving to database...")
        
        # 7-0. ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì‚­ì œ (ì¬ë¶„ì„ ì‹œ ì¤‘ë³µ ë°©ì§€)
        print("ğŸ—‘ï¸  ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ì‚­ì œ ì¤‘...")
        db.query(InterviewTranscript).filter(InterviewTranscript.video_id == video_id).delete()
        db.query(NonverbalMetrics).filter(NonverbalMetrics.video_id == video_id).delete()
        db.query(NonverbalTimeline).filter(NonverbalTimeline.video_id == video_id).delete()
        db.query(Feedback).filter(Feedback.video_id == video_id).delete()
        db.flush()  # ì‚­ì œë¥¼ ì¦‰ì‹œ ë°˜ì˜
        
        # 7-1. Transcript ì €ì¥
        transcript_record = InterviewTranscript(
            video_id=video_id,
            text=text,
            language="ko"  # Whisperê°€ ìë™ ê°ì§€í•˜ì§€ë§Œ ê¸°ë³¸ê°’
        )
        db.add(transcript_record)
        
        # 7-2. NonverbalMetrics ì €ì¥ (with metadata)
        metrics_record = NonverbalMetrics(
            video_id=video_id,
            center_gaze_ratio=metrics["center_gaze_ratio"],
            smile_ratio=metrics["smile_ratio"],
            nod_count=metrics["nod_count"],
            wpm=metrics["wpm"],
            filler_count=metrics["filler_count"],
            primary_emotion=primary_emo,
            metadata_json=json.dumps(metadata, ensure_ascii=False)  # Store metadata as JSON
        )
        db.add(metrics_record)
        
        # 7-3. NonverbalTimeline ì €ì¥
        timeline_record = NonverbalTimeline(
            video_id=video_id,
            timeline_json=json.dumps(timeline, ensure_ascii=False)
        )
        db.add(timeline_record)
        
        # 7-4. Feedback ì €ì¥
        feedback_records = []
        for idx, feedback_text in enumerate(feedback_list):
            # í”¼ë“œë°± ë¶„ë¥˜ (ê°„ë‹¨í•œ ê·œì¹™)
            if any(word in feedback_text for word in ["ìš°ìˆ˜", "ì•ˆì •ì ", "ìì—°ìŠ¤ëŸ½", "ì ì ˆ", "ê¸ì •ì "]):
                severity = "info"
                title = "ê°•ì "
            elif any(word in feedback_text for word in ["ê³¼ë‹¤", "ë§", "ë”±ë”±", "ë‚®", "ê¸´ì¥"]):
                severity = "warning"
                title = "ê°œì„  í•„ìš”"
            else:
                severity = "suggestion"
                title = "ì œì•ˆ"
            
            feedback_rec = Feedback(
                video_id=video_id,
                level="video",
                title=f"{title} #{idx+1}",
                message=feedback_text,
                severity=severity
            )
            feedback_records.append(feedback_rec)
            db.add(feedback_rec)
        
        # ì»¤ë°‹
        db.commit()
        
        print("âœ… Analysis complete!")
        
        return {
            "video_id": video_id,
            "metrics": {
                **metrics,
                "metadata": metadata  # Include computation metadata in response
            },
            "feedback": feedback_list,
            "feedback_mode": feedback_mode,
            "transcript": text,
            "database_records": {
                "transcript_id": transcript_record.id,
                "metrics_id": metrics_record.id,
                "timeline_id": timeline_record.id,
                "feedback_ids": [f.id for f in feedback_records]
            }
        }
    
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"Video analysis failed: {str(e)}"
        )


@router.get("/results/{video_id}")
def get_analysis_results(video_id: str, db: Session = Depends(get_db)):
    """
    ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
    
    Args:
        video_id: InterviewVideo ID
    
    Returns:
        ë¹„ë””ì˜¤, ë©”íŠ¸ë¦­, í”¼ë“œë°±, ì „ì‚¬ ë“± ëª¨ë“  ë¶„ì„ ê²°ê³¼
    """
    # ë¹„ë””ì˜¤ ì •ë³´
    video = db.query(InterviewVideo).filter(InterviewVideo.id == video_id).first()
    if not video:
        raise HTTPException(status_code=404, detail=f"Video not found: {video_id}")
    
    # ë©”íŠ¸ë¦­ (ê°€ì¥ ìµœì‹  ê²ƒ ì¡°íšŒ)
    metrics = db.query(NonverbalMetrics).filter(
        NonverbalMetrics.video_id == video_id
    ).order_by(NonverbalMetrics.created_at.desc()).first()
    
    # í”¼ë“œë°±
    feedbacks = db.query(Feedback).filter(Feedback.video_id == video_id).all()
    
    # ì „ì‚¬
    transcript = db.query(InterviewTranscript).filter(InterviewTranscript.video_id == video_id).first()
    
    # íƒ€ì„ë¼ì¸ (ê°€ì¥ ìµœì‹  ê²ƒ ì¡°íšŒ)
    timeline = db.query(NonverbalTimeline).filter(
        NonverbalTimeline.video_id == video_id
    ).order_by(NonverbalTimeline.created_at.desc()).first()
    
    # íƒ€ì„ë¼ì¸ì—ì„œ emotion_distribution ê³„ì‚°
    emotion_dist = {}
    primary_emo = None
    timeline_data = None
    if timeline:
        try:
            timeline_data = json.loads(timeline.timeline_json)
            emotion_dist = emotion_distribution(timeline_data)
            primary_emo = get_primary_emotion(timeline_data)
        except Exception as e:
            print(f"âš ï¸ íƒ€ì„ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
    
    # metricsì˜ primary_emotionì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if metrics and metrics.primary_emotion:
        primary_emo = metrics.primary_emotion
    
    # Parse metadata from JSON
    metadata_dict = None
    if metrics and metrics.metadata_json:
        try:
            metadata_dict = json.loads(metrics.metadata_json)
        except Exception as e:
            print(f"âš ï¸ ë©”íƒ€ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
    
    return {
        "video": {
            "id": video.id,
            "user_id": video.user_id,
            "session_id": video.session_id,
            "question_id": video.question_id,
            "duration_sec": video.duration_sec,
            "created_at": video.created_at
        },
        "metrics": {
            "center_gaze_ratio": metrics.center_gaze_ratio if metrics else None,
            "smile_ratio": metrics.smile_ratio if metrics else None,
            "nod_count": metrics.nod_count if metrics else None,
            "wpm": metrics.wpm if metrics else None,
            "filler_count": metrics.filler_count if metrics else None,
            "primary_emotion": primary_emo,
            "emotion_distribution": emotion_dist,
            "metadata": metadata_dict  # Include computation metadata
        } if metrics else None,
        "feedbacks": [
            {
                "id": f.id,
                "title": f.title,
                "message": f.message,
                "severity": f.severity,
                "level": f.level
            } for f in feedbacks
        ],
        "transcript": transcript.text if transcript else None,
        "timeline": timeline_data,  # íƒ€ì„ë¼ì¸ JSON ì§ì ‘ ë°˜í™˜
        "timeline_available": timeline is not None
    }
