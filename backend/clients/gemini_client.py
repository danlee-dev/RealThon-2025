"""
Gemini LLM ν΄λΌμ΄μ–ΈνΈ

π΅ A6000 λ§μ΄κ·Έλ μ΄μ… λ€μƒ
- ν„μ¬: Google Gemini API (λ¬΄λ£ ν‹°μ–΄)
- ν–¥ν›„: A6000 μ„λ²„μ LLaMA/Gemma (λ¬΄λ£, λ΅μ»¬)
"""

import os
import json
from typing import Optional
import google.generativeai as genai
from clients.base import LLMClient


class GeminiClient(LLMClient):
    """
    Google Gemini API ν΄λΌμ΄μ–ΈνΈ (κΌ¬λ¦¬μ§λ¬Έ μƒμ„±μ©)

    β οΈ A6000 μ„λ²„λ΅ λ§μ΄κ·Έλ μ΄μ… μ‹ LLaMAA6000Clientλ΅ κµμ²΄

    ν™κ²½ λ³€μ:
        GEMINI_API_KEY: Google Gemini API ν‚¤
        GEMINI_MODEL: λ¨λΈ μ΄λ¦„ (κΈ°λ³Έ: gemini-2.0-flash-exp)
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model_name: Optional[str] = None
    ):
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY is required")

        genai.configure(api_key=self.api_key)

        self.model_name = model_name or os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp")
        self.model = genai.GenerativeModel(self.model_name)

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 200,
        temperature: float = 0.7
    ) -> str:
        """
        ν”„λ΅¬ν”„νΈλ¥Ό λ°›μ•„ ν…μ¤νΈ μƒμ„±

        Args:
            prompt: μƒμ„± ν”„λ΅¬ν”„νΈ
            max_tokens: μµλ€ ν† ν° μ
            temperature: μƒμ„± μ¨λ„

        Returns:
            μƒμ„±λ ν…μ¤νΈ
        """
        generation_config = genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature
        )

        response = self.model.generate_content(
            prompt,
            generation_config=generation_config
        )

        return response.text.strip()

    def build_followup_prompt(
        self,
        portfolio_text: str,
        current_question: str,
        user_answer: str,
        question_type: str = "technical"
    ) -> str:
        """
        κΌ¬λ¦¬μ§λ¬Έ μƒμ„±μ„ μ„ν• ν”„λ΅¬ν”„νΈ κµ¬μ„±

        Args:
            portfolio_text: ν¬νΈν΄λ¦¬μ¤ νμ‹± ν…μ¤νΈ
            current_question: ν„μ¬ μ§λ¬Έ
            user_answer: μ‚¬μ©μ λ‹µλ³€ (STT κ²°κ³Ό)
            question_type: μ§λ¬Έ μ ν• (technical, behavioral, project)

        Returns:
            κΌ¬λ¦¬μ§λ¬Έ μƒμ„± ν”„λ΅¬ν”„νΈ
        """
        prompt = f"""λ‹Ήμ‹ μ€ κ²½ν— λ§μ€ κ°λ°μ λ©΄μ ‘κ΄€μ…λ‹λ‹¤.
μ§€μ›μμ ν¬νΈν΄λ¦¬μ¤μ™€ λ‹µλ³€μ„ κΈ°λ°μΌλ΅ κΉμ΄ μλ” κΌ¬λ¦¬μ§λ¬Έ 1κ°λ¥Ό μƒμ„±ν•΄μ£Όμ„Έμ”.

# ν¬νΈν΄λ¦¬μ¤ μ •λ³΄
{portfolio_text[:1000]}  # λ„λ¬΄ κΈΈλ©΄ μλΌλƒ„

# ν„μ¬ μ§λ¬Έ
{current_question}

# μ§€μ›μ λ‹µλ³€
{user_answer}

# μ”κµ¬μ‚¬ν•­
1. μ§€μ›μμ λ‹µλ³€μ—μ„ **κµ¬μ²΄μ μΈ κ²½ν—**μ„ λ” νκ³ λ“¤μ–΄μ•Ό ν•©λ‹λ‹¤
2. ν¬νΈν΄λ¦¬μ¤μ— μ–ΈκΈ‰λ κΈ°μ  μ¤νƒκ³Ό μ—°κ΄€μ§€μ–΄ μ§λ¬Έν•μ„Έμ”
3. λ‹¨λ‹µν•μ΄ μ•„λ‹, κ²½ν—κ³Ό μƒκ°μ„ μ΄λμ–΄λ‚Ό μ μλ” μ§λ¬Έμ΄μ–΄μ•Ό ν•©λ‹λ‹¤
4. μ§λ¬Έμ€ **1λ¬Έμ¥**μΌλ΅ κ°„κ²°ν•κ² μ‘μ„±ν•μ„Έμ”
5. μ΅΄λ“λ§μ„ μ‚¬μ©ν•μ„Έμ” (μ: "~ν•μ…¨λ‚μ”?", "~ν•μ‹  κ²½ν—μ΄ μλ‚μ”?")

# μ¶λ ¥ ν•μ‹
κΌ¬λ¦¬μ§λ¬Έλ§ μ¶λ ¥ν•μ„Έμ”. μ¶”κ°€ μ„¤λ…μ΄λ‚ λ§ν¬λ‹¤μ΄ μ—†μ΄ μ§λ¬Έ 1κ°λ§ μ‘μ„±ν•μ„Έμ”.

μμ‹:
- "λ°©κΈ λ§μ”€ν•μ‹  λ§μ΄ν¬λ΅μ„λΉ„μ¤ μ•„ν‚¤ν…μ²μ—μ„ μ„λΉ„μ¤ κ°„ ν†µμ‹ μ€ μ–΄λ–»κ² κµ¬ν„ν•μ…¨λ‚μ”?"
- "ν¬νΈν΄λ¦¬μ¤μ— μλ” Dockerλ¥Ό μ‹¤μ  ν”„λ΅μ νΈμ—μ„ μ–΄λ–»κ² ν™μ©ν•μ…¨λ‚μ”?"
"""
        return prompt


class LLaMAA6000Client(LLMClient):
    """
    A6000 μ„λ²„μ LLaMA/Gemma ν΄λΌμ΄μ–ΈνΈ

    β… μµμΆ… λ°°ν¬μ©

    ν™κ²½ λ³€μ:
        A6000_LLM_URL: A6000 LLM μ„λ²„ URL (μ: http://a6000-server:8003)
    """

    def __init__(self, base_url: Optional[str] = None):
        self.base_url = base_url or os.getenv("A6000_LLM_URL")
        if not self.base_url:
            raise ValueError("A6000_LLM_URL is required")

    async def generate(
        self,
        prompt: str,
        max_tokens: int = 200,
        temperature: float = 0.7
    ) -> str:
        """
        A6000 μ„λ²„μ LLMμΌλ΅ ν…μ¤νΈ μƒμ„±

        Args:
            prompt: μƒμ„± ν”„λ΅¬ν”„νΈ
            max_tokens: μµλ€ ν† ν° μ
            temperature: μƒμ„± μ¨λ„

        Returns:
            μƒμ„±λ ν…μ¤νΈ
        """
        import aiohttp

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/generate",
                json={
                    "prompt": prompt,
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
            ) as resp:
                resp.raise_for_status()
                result = await resp.json()
                return result.get("text", "")

    def build_followup_prompt(
        self,
        portfolio_text: str,
        current_question: str,
        user_answer: str,
        question_type: str = "technical"
    ) -> str:
        """GeminiClientμ™€ λ™μΌν• ν”„λ΅¬ν”„νΈ λΉλ”"""
        # GeminiClientμ™€ λ™μΌν• λ΅μ§
        prompt = f"""λ‹Ήμ‹ μ€ κ²½ν— λ§μ€ κ°λ°μ λ©΄μ ‘κ΄€μ…λ‹λ‹¤.
μ§€μ›μμ ν¬νΈν΄λ¦¬μ¤μ™€ λ‹µλ³€μ„ κΈ°λ°μΌλ΅ κΉμ΄ μλ” κΌ¬λ¦¬μ§λ¬Έ 1κ°λ¥Ό μƒμ„±ν•΄μ£Όμ„Έμ”.

# ν¬νΈν΄λ¦¬μ¤ μ •λ³΄
{portfolio_text[:1000]}

# ν„μ¬ μ§λ¬Έ
{current_question}

# μ§€μ›μ λ‹µλ³€
{user_answer}

# μ”κµ¬μ‚¬ν•­
1. μ§€μ›μμ λ‹µλ³€μ—μ„ **κµ¬μ²΄μ μΈ κ²½ν—**μ„ λ” νκ³ λ“¤μ–΄μ•Ό ν•©λ‹λ‹¤
2. ν¬νΈν΄λ¦¬μ¤μ— μ–ΈκΈ‰λ κΈ°μ  μ¤νƒκ³Ό μ—°κ΄€μ§€μ–΄ μ§λ¬Έν•μ„Έμ”
3. λ‹¨λ‹µν•μ΄ μ•„λ‹, κ²½ν—κ³Ό μƒκ°μ„ μ΄λμ–΄λ‚Ό μ μλ” μ§λ¬Έμ΄μ–΄μ•Ό ν•©λ‹λ‹¤
4. μ§λ¬Έμ€ **1λ¬Έμ¥**μΌλ΅ κ°„κ²°ν•κ² μ‘μ„±ν•μ„Έμ”
5. μ΅΄λ“λ§μ„ μ‚¬μ©ν•μ„Έμ”

# μ¶λ ¥
κΌ¬λ¦¬μ§λ¬Έλ§ μ¶λ ¥ν•μ„Έμ”.
"""
        return prompt
